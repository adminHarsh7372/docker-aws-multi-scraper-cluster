# docker-aws-multi-scraper-cluster

A simple **parallel web scraping infrastructure** using Docker containers, deployable on **AWS EC2 Free Tier**.  
This project demonstrates **scalability, containerization, and cloud deployment**


---


## âš¡ Project Overview

This project runs **three independent web scrapers side by side**, each scraping a different website and extracting only the **page `<title>`**.  
All scrapers share a **host-mounted `data/results/` folder** where results are stored as JSON.

| Scraper | Target URL |
|---------|------------|
| scraper-1 | https://example.com |
| scraper-2 | https://www.wikipedia.org |
| scraper-3 | https://www.python.org |

Each scraper runs in its **own Docker container**, making it **parallel, scalable, and cloud-ready**.


---


## ğŸ§© Architecture Diagram

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚        Host / AWS EC2        â”‚
     â”‚                              â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   â”‚ Scraper1  â”‚   â”‚ Scraper2 â”‚
     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚             â”‚    |
     â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”â”€â”€â”€â”€â”€â”˜    |  
     â”‚                   â”‚          |
     â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    |
     â”‚             â”‚ Scraper3  â”‚    |
     â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    |
     â”‚                   â”‚          |
     â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚             â”‚ /data/results â”‚ <- JSON output
     â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---


## ğŸ—‚ï¸ Folder Structure

docker-aws-multi-scraper-cluster/
â”œâ”€â”€ scraper-1/
â”‚ â”œâ”€â”€ scrape.py
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ scraper-2/
â”‚ â”œâ”€â”€ scrape.py
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ scraper-3/
â”‚ â”œâ”€â”€ scrape.py
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚ â””â”€â”€ results/
â””â”€â”€ docker-compose.yml


---


## ğŸš€ Running Locally

Clone the repository:
git clone https://github.com/adminHarsh7372/docker-aws-multi-scraper-cluster.git
cd docker-aws-multi-scraper-cluster

Create the data folder for results:
mkdir -p data/results

Run all scrapers:
docker compose up --build

Check results:
ls data/results/
cat data/results/scraper1.json
cat data/results/scraper2.json
cat data/results/scraper3.json
Each JSON file contains:

{
  "url": "https://www.python.org",
  "title": "Welcome to Python.org"
}


---



## â˜ï¸ Deploying to AWS EC2 (Free Tier)
Launch an Ubuntu 22.04 LTS EC2 instance (t2.micro - Free Tier).

Open port 22 (SSH) and optionally port 8000 for demo.

Connect via SSH:

ssh -i your-key.pem ubuntu@YOUR_EC2_IP

##Install Docker & Compose:

sudo apt update && sudo apt install -y docker.io docker-compose-plugin
sudo systemctl enable docker
sudo usermod -aG docker $USER
newgrp docker


Clone the repository:
git clone https://github.com/adminHarsh7372/docker-aws-multi-scraper-cluster.git
cd docker-aws-multi-scraper-cluster
mkdir -p data/results

Run scrapers:

docker compose up --build
View results:

ls data/results/
cat data/results/scraper1.json


---


## ğŸ“¦ How it Works
Docker Containers: Each scraper runs in its own isolated container.

Parallel Execution: Docker Compose launches all containers simultaneously.

Shared Volume: /data/results is shared between host and containers for easy result collection.

Minimal Scraping: Only fetches HTML and extracts the <title> tag using requests + regex.

##This project demonstrates:

Containerized scraping workflows

Parallel and scalable scraper execution

AWS Cloud deployment using Free Tier


---


## ğŸ·ï¸ Tech Stack
Python 3.11

Docker + Docker Compose

AWS EC2 Free Tier (Ubuntu 22.04 LTS)

Requests library for HTTP fetches

Minimal regex-based HTML parsing


